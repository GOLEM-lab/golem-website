{"title":"statistical entropy analysis of network data","markdown":{"yaml":{"author":"Termeh Shafie","date":"2022-10-01","title":"statistical entropy analysis of network data","image":"featured.png","title-block-style":"none","toc":true,"priority":1,"description":"A novel way to analyse multivariate network data. R package.","categories":["entropy"]},"headingText":"project summary","containsRefs":false,"markdown":"\n\n\nIn multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analysing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks. In this presentation,  multivariate entropy analysis is introduced and demonstrated as a general statistical method for finding, analysing and testing complicated dependence structures such as partial and conditional independencies, redundancies and functional dependencies.  For example, consider the joint entropies of all pairs of variables which are used to construct a sequence of association graphs that represent variables by nodes and pairwise dependences above decreasing thresholds by links (cf.  graphical models).  By successively lowering the threshold from the maximum joint entropy to smaller occurring values, the sequence of graphs get more and more links. Connected components that are cliques represent dependent subsets of variables, and different components represent independent subsets of variables. Conditional independence between subsets of variables can be identified by omitting the subset corresponding to the conditioning variables. By comparing such graphs given different thresholds and with different components and cliques, specific structural models of multivariate dependence can be suggested and tested by divergence measures of goodness of fit. The roles of  various entropy based measures are further highlighted and illustrated by applications on social network data.  These applications show that important social phenomena and processes are often identified using these tools. The proposed framework is implemented in the R package 'netropy' and a description of various functions implemented in the package\nare given in the following. More details are provided in the package\nvignettes and the references listed.\n\n\n## r package\n\nPackage overview: `netropy` <img src=\"hex_netropy.png\" align=\"right\" width=\"300px\"/>\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/netropy)](https://cran.r-project.org/package=netropy)\n[![CRAN\nDownloads](http://cranlogs.r-pkg.org/badges/netropy)](https://CRAN.R-project.org/package=netropy)\n\n### Installation\n\nYou can install the released version of netropy from\n[CRAN](https://CRAN.R-project.org) with:\n\n``` r\ninstall.packages(\"netropy\")\n```\n\nThe development version from [GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/netropy\")\n```\n``` r\nlibrary('netropy')\n```\n\n\n### Loading Internal Data\nThe different entropy tools are explained and illustrated by exploring\ndata from a network study of a corporate law firm, which has previously\nbeen analysed by several authors\n([link](https://www.stats.ox.ac.uk/~snijders/siena/Lazega_lawyers_data.htm)).\nThe data set is included in the package as a list with objects\nrepresenting adjacency matrices for each of the three networks advice\n(directed), friendship (directed) and co-work (undirected), together\nwith a data frame comprising 8 attributes on each of the 71 lawyers.\n\nTo load the data, extract each object and assign the correct names to\nthem:\n\n``` r\ndata(lawdata) \nadj.advice <- lawdata[[1]]\nadj.friend <- lawdata[[2]]\nadj.cowork <-lawdata[[3]]\ndf.att <- lawdata[[4]]\n```\n\n### Variable Domains and Data Editing\n\nA requirement for the applicability of these entropy tools is the\nspecification of discrete variables with finite range spaces on the same\ndomain: either node attributes/vertex variables, edges/dyad variables or\ntriad variables. These can be either observed or transformed as shown in\nthe following using the above example data set.\n\nWe have 8 vertex variables with 71 observations, two of which (`years`\nand `age`) are numerical and needs categorization based on their\ncumulative distributions. This categorization is in details described in\nthe vignette “variable domains and data editing”. Here we just show the\nnew dataframe created (note that variable `senior` is omitted as it only\ncomprises unique values and that we edit all variable to start from 0):\n\n``` r\natt.var <-\n  data.frame(\n    status   = df.att$status-1,\n    gender   = df.att$gender,\n    office   = df.att$office-1,\n    years    = ifelse(df.att$years <= 3,0,\n                      ifelse(df.att$years <= 13,1,2)),\n    age      = ifelse(df.att$age <= 35,0,\n                      ifelse(df.att$age <= 45,1,2)),\n    practice = df.att$practice,\n    lawschool= df.att$lawschool-1\n    )\nhead(att.var)\n#>   status gender office years age practice lawschool\n#> 1      0      1      0     2   2        1         0\n#> 2      0      1      0     2   2        0         0\n#> 3      0      1      1     1   2        1         0\n#> 4      0      1      0     2   2        0         2\n#> 5      0      1      1     2   2        1         1\n#> 6      0      1      1     2   2        1         0\n```\n\nThese vertex variables can be transformed into dyad variables by using\nthe function `get_dyad_var()`. Observed node attributes in the dataframe\n`att_var` are then transformed into pairs of individual attributes. For\nexample, `status` with binary outcomes is transformed into dyads having\n4 possible outcomes (0,0), (0,1), (1,0), (1,1):\n\n``` r\ndyad.status    <- get_dyad_var(att.var$status, type = 'att')\ndyad.gender    <- get_dyad_var(att.var$gender, type = 'att')\ndyad.office    <- get_dyad_var(att.var$office, type = 'att')\ndyad.years     <- get_dyad_var(att.var$years, type = 'att')\ndyad.age       <- get_dyad_var(att.var$age, type = 'att')\ndyad.practice  <- get_dyad_var(att.var$practice, type = 'att')\ndyad.lawschool <- get_dyad_var(att.var$lawschool, type = 'att')\n```\n\nSimilarly, dyad variables can be created based on observed ties. For the\nundirected edges, we use indicator variables read directly from the\nadjacency matrix for the dyad in question, while for the directed ones\n(`advice` and `friendship`) we have pairs of indicators representing\nsending and receiving ties with 4 possible outcomes :\n\n``` r\ndyad.cwk    <- get_dyad_var(adj.cowork, type = 'tie')\ndyad.adv    <- get_dyad_var(adj.advice, type = 'tie')\ndyad.frn    <- get_dyad_var(adj.friend, type = 'tie')\n```\n\nAll 10 dyad variables are merged into one data frame for subsequent\nentropy analysis:\n\n``` r\ndyad.var <-\n  data.frame(cbind(status   = dyad.status$var,\n                  gender    = dyad.gender$var,\n                  office    = dyad.office$var,\n                  years     = dyad.years$var,\n                  age       = dyad.age$var,\n                  practice  = dyad.practice$var,\n                  lawschool = dyad.lawschool$var,\n                  cowork    = dyad.cwk$var,\n                  advice    = dyad.adv$var,\n                  friend    = dyad.frn$var)\n                  )\nhead(dyad.var)\n#>   status gender office years age practice lawschool cowork advice friend\n#> 1      3      3      0     8   8        1         0      0      3      2\n#> 2      3      3      3     5   8        3         0      0      0      0\n#> 3      3      3      3     5   8        2         0      0      1      0\n#> 4      3      3      0     8   8        1         6      0      1      2\n#> 5      3      3      0     8   8        0         6      0      1      1\n#> 6      3      3      1     7   8        1         6      0      1      1\n```\n\nA similar function `get_triad_var()` is implemented for transforming\nvertex variables and different relation types into triad variables. This\nis described in more detail in the vignette “variable domains and data\nediting”.\n\n### Univariate, Bivariate and Trivariate Entropies\n\nThe function `entropy_bivar()` computes the bivariate entropies of all\npairs of variables in the dataframe. The output is given as an upper\ntriangular matrix with cells giving the bivariate entropies of row and\ncolumn variables. The diagonal thus gives the univariate entropies for\neach variable in the dataframe:\n\n``` r\nH2 <- entropy_bivar(dyad.var)\nH2\n#>           status gender office years   age practice lawschool cowork advice\n#> status     1.493  2.868  3.640 3.370 3.912    3.453     4.363  2.092  2.687\n#> gender        NA  1.547  3.758 3.939 4.274    3.506     4.439  2.158  2.785\n#> office        NA     NA  2.239 4.828 4.901    4.154     5.058  2.792  3.388\n#> years         NA     NA     NA 2.671 4.857    4.582     5.422  3.268  3.868\n#> age           NA     NA     NA    NA 2.801    4.743     5.347  3.411  4.028\n#> practice      NA     NA     NA    NA    NA    1.962     4.880  2.530  3.127\n#> lawschool     NA     NA     NA    NA    NA       NA     2.953  3.567  4.186\n#> cowork        NA     NA     NA    NA    NA       NA        NA  0.615  1.687\n#> advice        NA     NA     NA    NA    NA       NA        NA     NA  1.248\n#> friend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n#>           friend\n#> status     2.324\n#> gender     2.415\n#> office     3.044\n#> years      3.483\n#> age        3.637\n#> practice   2.831\n#> lawschool  3.812\n#> cowork     1.456\n#> advice     1.953\n#> friend     0.881\n```\n\nBivariate entropies can be used to detect redundant variables that\nshould be omitted from the dataframe for further analysis. This occurs\nwhen the univariate entropy for a variable is equal to the bivariate\nentropies for pairs including that variable. As seen above, the\ndataframe `dyad.var` has no redundant variables. This can also be\nchecked using the function `redundancy()` which yields a binary matrix\nas output indicating which row and column variables are hold the same\ninformation:\n\n``` r\nredundancy(dyad.var)\n#> no redundant variables\n#> NULL\n```\n\nMore examples of using the function `redundancy()` is given in the\nvignette “univariate bivariate and trivariate entropies”.\n\nTrivariate entropies can be computed using the function\n`entropy_trivar()` which returns a dataframe with the first three\ncolumns representing possible triples of variables `V1`,`V2`, and `V3`\nfrom the dataframe in question, and their entropies `H(V1,V2,V3)` as the\nfourth column. We illustrated this on the dataframe `dyad.var`:\n\n``` r\nH3 <- entropy_trivar(dyad.var)\nhead(H3, 10) # view first 10 rows of dataframe\n#>        V1     V2        V3 H(V1,V2,V3)\n#> 1  status gender    office       4.938\n#> 2  status gender     years       4.609\n#> 3  status gender       age       5.129\n#> 4  status gender  practice       4.810\n#> 5  status gender lawschool       5.664\n#> 6  status gender    cowork       3.464\n#> 7  status gender    advice       4.048\n#> 8  status gender    friend       3.685\n#> 9  status office     years       5.321\n#> 10 status office       age       5.721\n```\n\n### Joint Entropy and Association Graphs\n\nJoint entropies is a non-negative measure of association among pairs of\nvariables. It is equal to 0 if and only if two variables are completely\nindependent of each other.\n\nThe function `joint_entropy()` computes the joint entropies between all\npairs of variables in a given dataframe and returns a list consisting of\nthe upper triangular joint entropy matrix (univariate entropies in the\ndiagonal) and a dataframe giving the frequency distributions of unique\njoint entropy values. A function argument specifies the precision given\nin number of decimals for which the frequency distribution of unique\nentropy values is created (default is 3). Applying the function on the\ndataframe `dyad.var` with two decimals:\n\n``` r\nJ <- joint_entropy(dyad.var, 2)\nJ$matrix\n#>           status gender office years  age practice lawschool cowork advice\n#> status      1.49   0.17   0.09  0.79 0.38     0.00      0.08   0.02   0.05\n#> gender        NA   1.55   0.03  0.28 0.07     0.00      0.06   0.00   0.01\n#> office        NA     NA   2.24  0.08 0.14     0.05      0.13   0.06   0.10\n#> years         NA     NA     NA  2.67 0.61     0.05      0.20   0.02   0.05\n#> age           NA     NA     NA    NA 2.80     0.02      0.41   0.01   0.02\n#> practice      NA     NA     NA    NA   NA     1.96      0.04   0.05   0.08\n#> lawschool     NA     NA     NA    NA   NA       NA      2.95   0.00   0.01\n#> cowork        NA     NA     NA    NA   NA       NA        NA   0.62   0.18\n#> advice        NA     NA     NA    NA   NA       NA        NA     NA   1.25\n#> friend        NA     NA     NA    NA   NA       NA        NA     NA     NA\n#>           friend\n#> status      0.05\n#> gender      0.01\n#> office      0.08\n#> years       0.07\n#> age         0.05\n#> practice    0.01\n#> lawschool   0.02\n#> cowork      0.04\n#> advice      0.18\n#> friend      0.88\nJ$freq\n#>       j  #(J = j) #(J >= j)\n#> 1  0.79         1         1\n#> 2  0.61         1         2\n#> 3  0.41         1         3\n#> 4  0.38         1         4\n#> 5  0.28         1         5\n#> 6   0.2         1         6\n#> 7  0.18         2         8\n#> 8  0.17         1         9\n#> 9  0.14         1        10\n#> 10 0.13         1        11\n#> 11  0.1         1        12\n#> 12 0.09         1        13\n#> 13 0.08         4        17\n#> 14 0.07         2        19\n#> 15 0.06         2        21\n#> 16 0.05         7        28\n#> 17 0.04         2        30\n#> 18 0.03         1        31\n#> 19 0.02         5        36\n#> 20 0.01         5        41\n#> 21    0         4        45\n```\n\nAs seen, the strongest association is between the variables `status` and\n`years` with joint entropy values of 0.79. We have independence (joint\nentropy value of 0) between two pairs of variables:\n(`status`,`practice`), (`practise`,`gender`), (`cowork`,`gender`),and\n(`cowork`,`lawschool`).\n\nThese results can be illustrated in a association graph using the\nfunction `assoc_graph()` which returns a `ggraph` object in which nodes\nrepresent variables and links represent strength of association (thicker\nlinks indicate stronger dependence). To use the function we need to load\nthe `ggraph` library and to determine a threshold which the graph drawn\nis based on. We set it to 0.15 so that we only visualize the strongest\nassociations\n\n``` r\nlibrary(ggraph)\nassoc_graph(dyad.var, 0.15)\n```\n\n<img src=\"man/figures/README-assoc_g-1.png\" width=\"100%\" style=\"display: block; margin: auto;\" />\n\nGiven this threshold, we see isolated and disconnected nodes\nrepresenting independent variables. We note strong dependence between\nthe three dyadic variables `status`,`years` and `age`, but also a\nsomewhat strong dependence among the three variables `lawschool`,\n`years` and `age`, and the three variables `status`, `years` and\n`gender`. The association graph can also be interpreted as a tendency\nfor relations `cowork` and `friend` to be independent conditionally on\nrelation `advice`, that is, any dependence between dyad variables\n`cowork` and `friend` is explained by `advice`.\n\nA threshold that gives a graph with reasonably many small independent or\nconditionally independent subsets of variables can be considered to\nrepresent a multivariate model for further testing.\n\nMore details and examples of joint entropies and association graphs are\ngiven in the vignette “joint entropies and association graphs”.\n\n### Prediction Power Based on Expected Conditional Entropies\n\nThe function `prediction_power()` computes prediction power when pairs\nof variables in a given dataframe are used to predict a third variable\nfrom the same dataframe. The variable to be predicted and the dataframe\nin which this variable also is part of is given as input arguments, and\nthe output is an upper triangular matrix giving the expected conditional\nentropies of pairs of row and column variables (denoted *X* and *Y*) of\nthe matrix, i.e. *EH(Z\\|X,Y)* where *Z* is the variable to be predicted.\nThe diagonal gives *EH(Z\\|X)* , that is when only one variable as a\npredictor. Note that `NA`’s are in the row and column representing the\nvariable being predicted.\n\nAssume we are interested in predicting variable `status` (that is\nwhether a lawyer in the data set is an associate or partner). This is\ndone by running the following syntax\n\n``` r\nprediction_power('status', dyad.var)\n#>           status gender office years   age practice lawschool cowork advice\n#> status        NA     NA     NA    NA    NA       NA        NA     NA     NA\n#> gender        NA  1.375  1.180 0.670 0.855    1.304     1.225  1.306  1.263\n#> office        NA     NA  2.147 0.493 0.820    1.374     1.245  1.373  1.325\n#> years         NA     NA     NA 2.265 0.573    0.682     0.554  0.691  0.667\n#> age           NA     NA     NA    NA 1.877    1.089     0.958  1.087  1.052\n#> practice      NA     NA     NA    NA    NA    2.446     1.388  1.459  1.410\n#> lawschool     NA     NA     NA    NA    NA       NA     3.335  1.390  1.337\n#> cowork        NA     NA     NA    NA    NA       NA        NA  2.419  1.400\n#> advice        NA     NA     NA    NA    NA       NA        NA     NA  2.781\n#> friend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n#>           friend\n#> status        NA\n#> gender     1.270\n#> office     1.334\n#> years      0.684\n#> age        1.058\n#> practice   1.427\n#> lawschool  1.350\n#> cowork     1.411\n#> advice     1.407\n#> friend     3.408\n```\n\nFor better readability, the powers of different predictors can be\nconveniently compared by using prediction plots that display a color\nmatrix with rows for *X* and columns for *Y* with darker colors in the\ncells when we have higher prediction power for *Z*. This is shown for\nthe prediction of `status`:\n\n<img src=\"predplot-status.png\" align=\"center\"/>\n\nObviously, the darkest color is obtained when the variable to be\npredicted is included among the predictors, and the cells exhibit\nprediction power for a single predictor on the diagonal and for two\npredictors symmetrically outside the diagonal. Some findings are as\nfollows: good predictors for `status` are given by `years` in\ncombination with any other variable, and `age` in combination with any\nother variable. The best sole predictor is `gender`.\n\nMore details and examples of expected conditional entropies and\nprediction power are given in the vignette “prediction power based on\nexpected conditional entropies”.\n\n## References\n\nParts of the theoretical background is provided in the package\nvignettes, but for more details, consult the following literature:\n\n> Frank, O., & Shafie, T. (2016). Multivariate entropy analysis of\n> network data. *Bulletin of Sociological Methodology/Bulletin de\n> Méthodologie Sociologique*, 129(1), 45-63.\n> [link](https://doi.org/10.1177%2F0759106315615511)\n\n> Nowicki, K., Shafie, T., & Frank, O. (Forthcoming). *Statistical\n> Entropy Analysis of Network Data*."},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css","../../academicons.css"],"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.38","theme":["cosmo","../../custom.scss"],"author":"Termeh Shafie","date":"2022-10-01","title":"statistical entropy analysis of network data","image":"featured.png","title-block-style":"none","priority":1,"description":"A novel way to analyse multivariate network data. R package.","categories":["entropy"]},"extensions":{"book":{"multiFile":true}}}}}